#!/usr/bin/env python3
"""
æ•°æ®å¯¼å…¥å·¥å…· - ç”¨äºæ‰¹é‡å¯¼å…¥WireGuardé…ç½®å’Œç”¨æˆ·æ•°æ®
åŠŸèƒ½ç‰¹æ€§ï¼š
1. æ”¯æŒJSONå’ŒCSVæ ¼å¼æ•°æ®å¯¼å…¥
2. æ•°æ®éªŒè¯å’Œå»é‡
3. æ‰¹é‡å¯¼å…¥ä¼˜åŒ–
4. è¯¦ç»†çš„é”™è¯¯æ—¥å¿—è®°å½•
5. æ”¯æŒå¯¼å…¥é¢„è§ˆï¼ˆdry-runæ¨¡å¼ï¼‰
6. å¯¼å…¥è¿›åº¦æ˜¾ç¤º
"""
import sys
import os
import json
import csv
from pathlib import Path
from typing import Dict, List, Optional, Any
import logging
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DataImporter:
    """æ•°æ®å¯¼å…¥å™¨ - ç»Ÿä¸€å¤„ç†å„ç§æ•°æ®å¯¼å…¥æ“ä½œ"""
    
    def __init__(self, database_url: str, dry_run: bool = False):
        """
        åˆå§‹åŒ–æ•°æ®å¯¼å…¥å™¨
        
        å‚æ•°:
            database_url: æ•°æ®åº“è¿æ¥URL
            dry_run: æ˜¯å¦ä¸ºé¢„è§ˆæ¨¡å¼ï¼ˆä¸å®é™…å†™å…¥æ•°æ®åº“ï¼‰
        """
        self.database_url = database_url
        self.dry_run = dry_run
        self.import_stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'skipped': 0
        }
    
    def import_json_file(self, file_path: str) -> bool:
        """
        ä»JSONæ–‡ä»¶å¯¼å…¥æ•°æ®
        
        å‚æ•°:
            file_path: JSONæ–‡ä»¶è·¯å¾„
        
        è¿”å›:
            æ˜¯å¦å¯¼å…¥æˆåŠŸ
        """
        logger.info(f"ğŸ“ å¼€å§‹å¯¼å…¥JSONæ–‡ä»¶: {file_path}")
        
        try:
            # è¯»å–JSONæ–‡ä»¶
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # éªŒè¯æ•°æ®æ ¼å¼
            if not isinstance(data, (list, dict)):
                logger.error("âŒ JSONæ•°æ®æ ¼å¼é”™è¯¯ï¼Œå¿…é¡»æ˜¯åˆ—è¡¨æˆ–å­—å…¸")
                return False
            
            # å¤„ç†å•ä¸ªå¯¹è±¡çš„æƒ…å†µ
            if isinstance(data, dict):
                data = [data]
            
            logger.info(f"ğŸ“Š æ‰¾åˆ° {len(data)} æ¡æ•°æ®è®°å½•")
            
            # æ‰¹é‡å¯¼å…¥æ•°æ®
            return self._batch_import(data)
            
        except FileNotFoundError:
            logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
            return False
        except Exception as e:
            logger.error(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
            return False
    
    def import_csv_file(self, file_path: str, column_mapping: Dict[str, str]) -> bool:
        """
        ä»CSVæ–‡ä»¶å¯¼å…¥æ•°æ®
        
        å‚æ•°:
            file_path: CSVæ–‡ä»¶è·¯å¾„
            column_mapping: åˆ—åæ˜ å°„å­—å…¸ï¼ˆCSVåˆ—å -> æ•°æ®åº“å­—æ®µåï¼‰
        
        è¿”å›:
            æ˜¯å¦å¯¼å…¥æˆåŠŸ
        """
        logger.info(f"ğŸ“ å¼€å§‹å¯¼å…¥CSVæ–‡ä»¶: {file_path}")
        
        try:
            # è¯»å–CSVæ–‡ä»¶
            data = []
            with open(file_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                
                for row in reader:
                    # æ ¹æ®æ˜ å°„è½¬æ¢å­—æ®µå
                    mapped_row = {}
                    for csv_col, db_field in column_mapping.items():
                        if csv_col in row:
                            mapped_row[db_field] = row[csv_col]
                    
                    data.append(mapped_row)
            
            logger.info(f"ğŸ“Š æ‰¾åˆ° {len(data)} æ¡æ•°æ®è®°å½•")
            
            # æ‰¹é‡å¯¼å…¥æ•°æ®
            return self._batch_import(data)
            
        except FileNotFoundError:
            logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False
        except Exception as e:
            logger.error(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
            return False
    
    def _batch_import(self, data: List[Dict[str, Any]]) -> bool:
        """
        æ‰¹é‡å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“
        
        å‚æ•°:
            data: è¦å¯¼å…¥çš„æ•°æ®åˆ—è¡¨
        
        è¿”å›:
            æ˜¯å¦å¯¼å…¥æˆåŠŸ
        """
        self.import_stats['total'] = len(data)
        
        if self.dry_run:
            logger.info("ğŸ” é¢„è§ˆæ¨¡å¼ï¼šä»¥ä¸‹æ˜¯å°†è¦å¯¼å…¥çš„æ•°æ®ï¼ˆä¸ä¼šå®é™…å†™å…¥æ•°æ®åº“ï¼‰")
        
        # éå†æ¯æ¡æ•°æ®è®°å½•
        for idx, record in enumerate(data, start=1):
            try:
                # éªŒè¯æ•°æ®
                if not self._validate_record(record):
                    logger.warning(f"âš ï¸ è®°å½• {idx} éªŒè¯å¤±è´¥ï¼Œè·³è¿‡")
                    self.import_stats['skipped'] += 1
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆå»é‡ï¼‰
                if self._check_duplicate(record):
                    logger.info(f"â„¹ï¸ è®°å½• {idx} å·²å­˜åœ¨ï¼Œè·³è¿‡")
                    self.import_stats['skipped'] += 1
                    continue
                
                # é¢„è§ˆæ¨¡å¼æˆ–å®é™…å¯¼å…¥
                if self.dry_run:
                    logger.info(f"ğŸ“ è®°å½• {idx}: {json.dumps(record, ensure_ascii=False)}")
                    self.import_stats['success'] += 1
                else:
                    # å®é™…æ’å…¥æ•°æ®åº“
                    if self._insert_record(record):
                        logger.info(f"âœ… è®°å½• {idx} å¯¼å…¥æˆåŠŸ")
                        self.import_stats['success'] += 1
                    else:
                        logger.error(f"âŒ è®°å½• {idx} å¯¼å…¥å¤±è´¥")
                        self.import_stats['failed'] += 1
                
                # æ˜¾ç¤ºè¿›åº¦
                if idx % 10 == 0:
                    progress = (idx / len(data)) * 100
                    logger.info(f"ğŸ“Š å¯¼å…¥è¿›åº¦: {progress:.1f}% ({idx}/{len(data)})")
                    
            except Exception as e:
                logger.error(f"âŒ å¤„ç†è®°å½• {idx} æ—¶å‡ºé”™: {e}")
                self.import_stats['failed'] += 1
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self._print_stats()
        
        # åˆ¤æ–­æ˜¯å¦æˆåŠŸï¼ˆè‡³å°‘æœ‰ä¸€åŠçš„è®°å½•å¯¼å…¥æˆåŠŸï¼‰
        return self.import_stats['success'] > 0
    
    def _validate_record(self, record: Dict[str, Any]) -> bool:
        """
        éªŒè¯å•æ¡è®°å½•çš„æ•°æ®æ ¼å¼
        
        å‚æ•°:
            record: è¦éªŒè¯çš„æ•°æ®è®°å½•
        
        è¿”å›:
            æ•°æ®æ˜¯å¦æœ‰æ•ˆ
        """
        # æ£€æŸ¥å¿…éœ€å­—æ®µï¼ˆæ ¹æ®å®é™…ä¸šåŠ¡éœ€æ±‚è°ƒæ•´ï¼‰
        required_fields = ['name']  # ç¤ºä¾‹ï¼šè‡³å°‘éœ€è¦åç§°å­—æ®µ
        
        for field in required_fields:
            if field not in record or not record[field]:
                logger.warning(f"âš ï¸ ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                return False
        
        return True
    
    def _check_duplicate(self, record: Dict[str, Any]) -> bool:
        """
        æ£€æŸ¥è®°å½•æ˜¯å¦å·²å­˜åœ¨ï¼ˆå»é‡æ£€æŸ¥ï¼‰
        
        å‚æ•°:
            record: è¦æ£€æŸ¥çš„æ•°æ®è®°å½•
        
        è¿”å›:
            æ˜¯å¦ä¸ºé‡å¤è®°å½•
        """
        # TODO: å®ç°å®é™…çš„æ•°æ®åº“æŸ¥è¯¢é€»è¾‘
        # è¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…åº”è¯¥æŸ¥è¯¢æ•°æ®åº“
        return False
    
    def _insert_record(self, record: Dict[str, Any]) -> bool:
        """
        æ’å…¥å•æ¡è®°å½•åˆ°æ•°æ®åº“
        
        å‚æ•°:
            record: è¦æ’å…¥çš„æ•°æ®è®°å½•
        
        è¿”å›:
            æ˜¯å¦æ’å…¥æˆåŠŸ
        """
        try:
            # TODO: å®ç°å®é™…çš„æ•°æ®åº“æ’å…¥é€»è¾‘
            # è¿™é‡Œåªæ˜¯ç¤ºä¾‹
            logger.debug(f"æ’å…¥è®°å½•: {record}")
            return True
        except Exception as e:
            logger.error(f"æ’å…¥è®°å½•å¤±è´¥: {e}")
            return False
    
    def _print_stats(self):
        """æ‰“å°å¯¼å…¥ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("=" * 50)
        logger.info("ğŸ“Š å¯¼å…¥ç»Ÿè®¡")
        logger.info("=" * 50)
        logger.info(f"æ€»è®°å½•æ•°: {self.import_stats['total']}")
        logger.info(f"æˆåŠŸå¯¼å…¥: {self.import_stats['success']}")
        logger.info(f"å¯¼å…¥å¤±è´¥: {self.import_stats['failed']}")
        logger.info(f"è·³è¿‡è®°å½•: {self.import_stats['skipped']}")
        
        if self.import_stats['total'] > 0:
            success_rate = (self.import_stats['success'] / self.import_stats['total']) * 100
            logger.info(f"æˆåŠŸç‡: {success_rate:.1f}%")
        
        logger.info("=" * 50)


def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    
    # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(
        description='æ•°æ®å¯¼å…¥å·¥å…· - æ‰¹é‡å¯¼å…¥WireGuardé…ç½®å’Œç”¨æˆ·æ•°æ®',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä»JSONæ–‡ä»¶å¯¼å…¥æ•°æ®
  python data_import_tool.py --json data.json
  
  # ä»CSVæ–‡ä»¶å¯¼å…¥æ•°æ®ï¼ˆéœ€è¦æŒ‡å®šåˆ—æ˜ å°„ï¼‰
  python data_import_tool.py --csv data.csv --mapping '{"csv_name":"name","csv_key":"public_key"}'
  
  # é¢„è§ˆæ¨¡å¼ï¼ˆä¸å®é™…å†™å…¥æ•°æ®åº“ï¼‰
  python data_import_tool.py --json data.json --dry-run
        """
    )
    
    parser.add_argument('--json', type=str, help='JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--csv', type=str, help='CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--mapping', type=str, help='CSVåˆ—åæ˜ å°„ï¼ˆJSONæ ¼å¼å­—ç¬¦ä¸²ï¼‰')
    parser.add_argument('--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…å†™å…¥æ•°æ®åº“')
    parser.add_argument('--database-url', type=str, help='æ•°æ®åº“è¿æ¥URLï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰')
    
    args = parser.parse_args()
    
    # è·å–æ•°æ®åº“URL
    database_url = args.database_url or os.getenv('DATABASE_URL')
    if not database_url:
        logger.error("âŒ æœªæŒ‡å®šæ•°æ®åº“URLï¼Œè¯·è®¾ç½®DATABASE_URLç¯å¢ƒå˜é‡æˆ–ä½¿ç”¨--database-urlå‚æ•°")
        return 1
    
    # åˆ›å»ºå¯¼å…¥å™¨
    importer = DataImporter(database_url, dry_run=args.dry_run)
    
    # æ‰§è¡Œå¯¼å…¥
    success = False
    
    if args.json:
        # å¯¼å…¥JSONæ–‡ä»¶
        success = importer.import_json_file(args.json)
    elif args.csv:
        # å¯¼å…¥CSVæ–‡ä»¶
        if not args.mapping:
            logger.error("âŒ å¯¼å…¥CSVæ–‡ä»¶éœ€è¦æŒ‡å®šåˆ—æ˜ å°„ï¼ˆ--mappingå‚æ•°ï¼‰")
            return 1
        
        try:
            # è§£æåˆ—æ˜ å°„
            column_mapping = json.loads(args.mapping)
            success = importer.import_csv_file(args.csv, column_mapping)
        except json.JSONDecodeError:
            logger.error("âŒ åˆ—æ˜ å°„æ ¼å¼é”™è¯¯ï¼Œå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONå­—ç¬¦ä¸²")
            return 1
    else:
        parser.print_help()
        return 1
    
    if success:
        logger.info("âœ… æ•°æ®å¯¼å…¥å®Œæˆ")
        return 0
    else:
        logger.error("âŒ æ•°æ®å¯¼å…¥å¤±è´¥")
        return 1


if __name__ == "__main__":
    sys.exit(main())
